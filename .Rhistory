head(data_1_raw, n = 5)
data_1_raw %>% summary()
# Use 1x3 grid
layout(t(1:3))
hist(data_1_raw$Y,  xlab = "Target Variable: Y", main = "Target Variable: Y (Claim Log Amts.)")
hist(data_1_raw$X1, xlab = "Predictor: X1",      main = "Predictor X1 (Annual Dist. Travelled)")
hist(data_1_raw$X2, xlab = "Predictor: X2",      main = "Predictor X2 (Average Dist. per Trip)")
# cor(data_1_raw, method = "pearson")
summarytools::descr(data_1_raw, stats = c("n.valid","pct.valid","mean","IQR","sd","CV","Skewness"))
ggplot(data_1_raw, aes(x=X1,y =X2,color=Y, size = 0.5)) +
geom_point() +
scale_color_gradient(low = "blue", high = "red") +
xlab("X1 (Annual Distance Travelled)") +
ylab("X2 (Average Distance per Trip)") +
ggtitle("Visualising Claim Log Amounts, by Predictors X1 and X2")+
scale_size(guide="none")+
theme(plot.title = element_text(hjust = 0.5))
# Description of data and Problem
# Target variable ..
# predictor variable
# correlation
# standardisation performed by glmnet prior to fitting
set.seed(111)
data_1 <- data_1_raw
data_1$Subset <- if_else(runif(nrow(data_1))<0.7,"Train","Test")
set.seed(111)
View(data_1)
420*0.3
m.m1       <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_1 %>%                                                             select(-Y,-Subset))
m.m1
Y          <- data_1$Y
head(m.m1, n = 5)
?model.matrix
m.m1_train <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_1 %>% filter(Subset == "Train") %>%                  select(-Y,-Subset))
m.m1_test  <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_1 %>% filter(Subset == "Test") %>%                   select(-Y,-Subset))
Y_train    <- data_1[data_1$Subset == "Train",]$Y
Y_test     <- data_1[data_1$Subset == "Test" ,]$Y
# Fit Model on training set
mod1.lasso <- glmnet(m.m1_train, Y_train, family = "gaussian", alpha = 1, weights = NULL, offset = NULL)
mod1.lasso
rmse_fun <- function(Y,YHat){sqrt(apply((Y - YHat)^2,2,mean)) %>% as.numeric()}
pred1.lasso_train <- predict(mod1.lasso, newx = m.m1_train)
rmse1_train = rmse_fun(Y_train,pred1.lasso_train)
min(rmse1_train)
# For Out-of sample test dataset
pred1.lasso_test <- predict(mod1.lasso, newx = m.m1_test)
rmse1_test = rmse_fun(Y_test,pred1.lasso_test)
min(rmse1_test)
Y_train
pred1.lasso_train
(c(1:3) - c(2:4))^2
(c(1:3) - c(1, 3,5))^2
rmse_fun <- function(Y,YHat){sqrt(mean(Y - YHat)^2)}
rmse1_train = rmse_fun(Y_train,pred1.lasso_train)
min(rmse1_train)
0^2
Y=Y_train
YHat
YHat=pred1.lasso_train
(Y - YHat)^2
YHat
# Define RMSE() function to caluculate 'Root Mean Square Error' model fit
# Inputs: actual (Y) and Expected (YHat)
rmse_fun <- function(Y,YHat){sqrt(apply((Y - YHat)^2,2,mean)) %>% as.numeric()}
pred1.lasso_train <- predict(mod1.lasso, newx = m.m1_train)
rmse1_train = rmse_fun(Y_train,pred1.lasso_train)
min(rmse1_train)
# For Out-of sample test dataset
pred1.lasso_test <- predict(mod1.lasso, newx = m.m1_test)
rmse1_test = rmse_fun(Y_test,pred1.lasso_test)
min(rmse1_test)
# Visualise the lasso routine
layout(t(1:2))
plot(mod1.lasso,xvar="lambda",label=TRUE)
plot(log(mod1.lasso$lambda),rmse1_test)
# Find best lambda and lasso model
lambda1.best    <- mod1.lasso$lambda[rmse1_test  == min(rmse1_test)]
mod1.lasso.best <- glmnet(m.m1_train, Y_train, family = "gaussian", alpha = 1, lambda = lambda1.best,                                weights = NULL, offset = NULL)
mod1.lasso.best
# Check the regularised coefficients, and see which one may be been dropped
coefficients(mod1.lasso.best)
coefficients(mod1.lasso.best)
coefficients(mod1.lasso.best) %>% as.matrix()
coefficients(mod1.lasso.best) %>% as.data.frame()
coefficients(mod1.lasso.best) %>% as.data.frame() %>% filter(s0 != 0)
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
Lasso1.Sig.Vars
Lasso1.Sig.Vars
head(m.m1)
colnames(m.m1) %in% Lasso1.Sig.Vars
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
View(m.m1_sig)
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
head(m.m1_sig)
coefficients(mod1.lasso.best)
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0)
# Quick visualisation of the new model matrix m.m1:
head(m.m1, n = 5)
coefficients(mod1.lasso.best)
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0)
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
colnames(m.m1)
colnames(m.m1) %in% Lasso1.Sig.Vars
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>%  as.data.frame() %>% filter(s0 != 0) %>% rownames()
coefficients(mod1.lasso.best) %>% as.matrix()
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame()
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0)
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% mutate(rownames_to_column())
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% mutate(rownames_to_column("x"))
?rownames_to_column("x") : is.data.frame(df) is not TRUE
?rownames_to_column
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = row_names)
coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = "row_names")
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = "row_names") filter(s0 != 0) %>% pull(row_names)
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = "row_names") %>% filter(s0 != 0) %>% pull(row_names)
Lasso1.Sig.Vars
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = "row_names") %>% filter(s0 != 0) %>% pull(row_names)
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
# Subset model matrices, for significant predictors:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column(var = "row_names") %>% filter(s0 != 0) %>% pull(row_names)
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
# Quick visualisation of the new model matrix m.m1:
head(m.m1, n = 5)
# Visualise the lasso routine
layout(t(1:2))
plot(mod1.lasso,xvar="lambda", label=TRUE)
plot(log(mod1.lasso$lambda), rmse1_test)
# Find best lambda and lasso model by finding lambda that minimises RMSE using the test dataset
lambda1.best    <- mod1.lasso$lambda[rmse1_test  == min(rmse1_test)]
mod1.lasso.best <- glmnet(m.m1_train, Y_train, family = "gaussian", alpha = 1, lambda = lambda1.best,                                weights = NULL, offset = NULL)
mod1.lasso.best
#Find predictors with non-zero coefficients:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %> %
as.data.frame() %>% rownames_to_column(var = "row_names") %>%
filter(s0 != 0) %>% pull(row_names)
#Find predictors with non-zero coefficients:
Lasso1.Sig.Vars <- coefficients(mod1.lasso.best) %>% as.matrix() %>%
as.data.frame() %>% rownames_to_column(var = "row_names") %>%
filter(s0 != 0) %>% pull(row_names)
#Subset model matrices, for significant predictors:
m.m1_sig       <-  m.m1[,colnames(m.m1) %in% Lasso1.Sig.Vars]
m.m1_train_sig <-  m.m1_train[,colnames(m.m1_train) %in% Lasso1.Sig.Vars]
m.m1_test_sig  <-  m.m1_test[ ,colnames(m.m1_test)  %in% Lasso1.Sig.Vars]
# Quick visualisation of the new model matrix m.m1_sig:
head(m.m1_sig, n = 5)
# Gap Statistic
gap_stat <- clusGap(data_1 %>% select(X1,X2),
FUNcluster = kmeans,
nstart  = 10,
K.max   = 10,
d.power = 1,
spaceH0 = "original",
verbose = FALSE
)
gap_stat_df <- gap_stat$Tab %>% as.data.frame()
# Obtain K ("K*") that maximizes Gap Statistic (within one standard error)
# Mathematically, this is to derive smallest K, such that G(K) >= G(K+1) - sâ€™_(K+1)
BestK <- maxSE(f         = gap_stat_df$gap,
SE.f      = gap_stat_df$SE.sim,
method    = c("firstSEmax"),
SE.factor = 1)
print(paste0("BestK, K* that optimises Gap statistic: K* = ",BestK))
WSS_Vec <- vector()
for (k in 1:10) {
k.means <- kmeans(data_1 %>% select(X1,X2),
centers = k,
iter.max = 1000,
nstart = 50,
algorithm = "Lloyd",
trace = FALSE)
WSS_Vec[k] <- k.means$tot.withinss
if (k == BestK) {
print(paste0("Assigning ", k, "-th iteration as best k.means clusters"))
k.means.best <- k.means}
}
# Visualise clustering statistics
layout(t(1:2))
# Elbow Method (Total-WSS)
plot(x = 1:10, y = WSS_Vec,
xlab = "Number of clusters, k",
ylab = "Total-WSS",
main = "Elbow method") +
lines(x=1:10, y=WSS_Vec) +
axis(side=1,at=c(1:10))
# Gap statistic
plot(gap_stat,
xlab = "Number of clusters, k",
ylab = "Gap to Ref.Model, G(K)",
main = "GapStatistic") +
axis(side=1,at=c(0:10))
# Elbow Method (Total-WSS)
plot(x = 1:10, y = WSS_Vec,
xlab = "Number of clusters, k",
ylab = "Total-WSS",
main = "Elbow method") +
lines(x=1:10, y=WSS_Vec) +
axis(side=1,at=c(1:10))
# Visualise clustering statistics
layout(t(1:2))
# Elbow Method (Total-WSS)
plot(x = 1:10, y = WSS_Vec,
xlab = "Number of clusters, k",
ylab = "Total-WSS",
main = "Elbow method")
lines(x=1:10, y=WSS_Vec)
axis(side=1,at=c(1:10))
# Gap statistic
plot(gap_stat,
xlab = "Number of clusters, k",
ylab = "Gap to Ref.Model, G(K)",
main = "GapStatistic") +
axis(side=1,at=c(0:10))
# Visualise clustering statistics
layout(t(1:2))
# Elbow Method (Total-WSS)
plot(x = 1:10, y = WSS_Vec,
xlab = "Number of clusters, k",
ylab = "Total-WSS",
main = "Elbow method") +
lines(x=1:10, y=WSS_Vec) +
axis(side=1,at=c(1:10))
# Gap statistic
plot(gap_stat,
xlab = "Number of clusters, k",
ylab = "Gap to Ref.Model, G(K)",
main = "GapStatistic") +
axis(side=1,at=c(0:10))
# Before binding cluster information, it is first converted to factor format
# This makes it easier for algorithm and us to work with
# glmnet will dummy-code the variables as necessary
data_2        <- data_1
data_2$Clust  <- k.means.best$cluster %>% as.factor()
head(data_1)
head(data_2)
svm_data_temp <- data_2 %>% select(-Y,-Subset)
svm(Clust ~.,
data = svm_data_temp ,
kernel="linear",
cost=10,
scale=FALSE) %>%
plot(svm_data_temp)
head(svm_data_temp)
# Create new model matrix, with suffix "m.m2"
m.m2       <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_2 %>%                                                select(-Y,-Subset))
head(m.m2, n = 5)
# Continue this logic to make train and test subsets
# Here we have chosen train/test splits to enable out-of-sample testing without K-fold Cross Validation (which will be covered later)
m.m2_train <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_2 %>%
filter(Subset == "Train") %>%
select(-Y,-Subset))
m.m2_test  <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,data = data_2 %>%
filter(Subset == "Test") %>%
select(-Y,-Subset))
# Continue this logic to make train and test subsets
# Here we have chosen train/test splits to enable out-of-sample testing without K-fold Cross Validation (which will be covered later)
m.m2_train <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = data_2 %>%
filter(Subset == "Train") %>%
select(-Y,-Subset))
m.m2_test  <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = data_2 %>%
filter(Subset == "Test") %>%
select(-Y,-Subset))
head(m.m2_test )
mod2.lasso <- glmnet(m.m2_train, Y_train, family = "gaussian", weights = NULL, offset = NULL, alpha = 1)
# Predict and score the model (using in- and out-of-sample RMSE)
# On Training set
pred2.lasso_train <- predict(mod2.lasso, newx = m.m2_train)
rmse2_train = rmse_fun(Y_train, pred2.lasso_train)
min(rmse2_train)
# On Test set
pred2.lasso_test <- predict(mod2.lasso, newx = m.m2_test)
rmse2_test = rmse_fun(Y_test, pred2.lasso_test)
min(rmse2_test)
# Visualise the lasso routine
layout(t(1:2))
plot(mod2.lasso, xvar="lambda", label=TRUE)
plot(log(mod2.lasso$lambda), rmse2_test)
# Find best lambda and lasso model (finding lambda that minimises RMSE from test dataset)
lambda2.best <- mod2.lasso$lambda[rmse2_test  == min(rmse2_test)]
mod2.lasso.best <- glmnet(m.m2_train, Y_train, family = "gaussian", weights = NULL, offset = NULL, alpha                             = 1, lambda = lambda2.best)
mod2.lasso.best
# Check the regularised coefficients, and see which one may be been dropped
coefficients(mod2.lasso.best)
Lasso2.Sig.Vars <- coefficients(mod2.lasso.best) %>% as.matrix() %>% as.data.frame() %>% filter(s0 != 0) %>% rownames()
Lasso2.Sig.Vars
Lasso2.Sig.Vars <- coefficients(mod2.lasso.best) %>% as.matrix() %>%
as.data.frame() %>% rownames_to_column(var = "row_names") %>%
filter(s0 != 0) %>% pull(row_names)
Lasso2.Sig.Vars
coefficients(mod2.lasso.best)
which(coefficients(mod2.lasso.best) !=0)
names(coefficients(mod2.lasso.best))
rownames(coefficients(mod2.lasso.best))
rownames(coefficients(mod2.lasso.best))[which(coefficients(mod2.lasso.best) !=0)]
#Find significant predictors (i.e. non-zero coefficient):
Lasso2.Sig.Vars <- coefficients(mod2.lasso.best) %>% as.matrix() %>%
as.data.frame() %>% rownames_to_column(var = "row_names") %>%
filter(s0 != 0) %>% pull(row_names)
# Subset model matrices for significant predictors:
m.m2_sig       <-  m.m2[,colnames(m.m2) %in% Lasso2.Sig.Vars]
m.m2_train_sig <-  m.m2_train[,colnames(m.m2_train) %in% Lasso2.Sig.Vars]
m.m2_test_sig  <-  m.m2_test[ ,colnames(m.m2_test)  %in% Lasso2.Sig.Vars]
# Quick visualisation of the new model matrix m.m2_sig:
head(m.m2_sig, n = 5)
#Find significant predictors (i.e. non-zero coefficient):
Lasso2.Sig.Vars <- coefficients(mod2.lasso.best) %>% as.matrix() %>%
as.data.frame() %>% rownames_to_column(var = "row_names") %>%
filter(s0 != 0) %>% pull(row_names)
# Subset model matrices for significant predictors:
m.m2_sig       <-  m.m2[,colnames(m.m2) %in% Lasso2.Sig.Vars]
m.m2_train_sig <-  m.m2_train[,colnames(m.m2_train) %in% Lasso2.Sig.Vars]
m.m2_test_sig  <-  m.m2_test[ ,colnames(m.m2_test)  %in% Lasso2.Sig.Vars]
Lasso2.Sig.Vars
colnames(m.m2)
# Set seed for K-fold cross validation
seed_cv <- 222
set.seed(seed_cv)
fit1_cv           <- cv.glmnet(m.m1_train_sig, Y_train, alpha=1, nfolds = 10)
fit1_parsimonious <- glmnet(m.m1_train_sig,
Y_train,
family = "gaussian",
weights = NULL,
offset = NULL,
alpha = 1,
lambda = fit1_cv$lambda.1se
)
fit1_cv
fit1_cv$lambda.1se
# In-sample
pred.fit1_train <- predict(fit1_parsimonious, newx = m.m1_train_sig)
rmse1_train = rmse_fun(Y_train, pred.fit1_train)
# Out-of-sample
pred.fit1_test <- predict(fit1_parsimonious, newx = m.m1_test_sig)
rmse1_train = rmse_fun(Y_test, pred.fit1_test)
fit1_cv$lambda.1se
set.seed(seed_cv)
fit2_cv           <- cv.glmnet(m.m2_train_sig, Y_train, alpha=1, nfolds = 10)
fit2_parsimonious <- glmnet( m.m2_train_sig,
Y_train,
family = "gaussian",
weights = NULL,
offset = NULL,
alpha = 1,
lambda = fit2_cv$lambda.1se
)
# In-sample
pred.fit2_train <- predict(fit2_parsimonious, newx = m.m2_train_sig)
rmse2_train     <- rmse_fun(Y_train, pred.fit2_train)
# Out-of-sample
pred.fit2_test <- predict(fit2_parsimonious, newx = m.m2_test_sig)
rmse2_train    <- rmse_fun(Y_test, pred.fit2_test)
# Gather key result to compare
tibble( "Type"    = c("In-sample","Out-of sample")
,"NoClus"  = c(min(rmse1_train),min(rmse1_test))
,"ClusSig" = c(min(rmse2_train),min(rmse2_test))
) %>% knitr::kable()
# Gather key result to compare
tibble( "Type"    = c("In-sample","Out-of-sample")
,"NoClus"  = c(min(rmse1_train),min(rmse1_test))
,"ClusSig" = c(min(rmse2_train),min(rmse2_test))
) %>% knitr::kable()
set.seed(333)
num_n <-  10
num_k <-  10
rec_vec = matrix(nrow = num_n,
ncol = num_k)
# Cycle through re-samples n
for (n in 1:num_n) {
# Chose different seed for each random subset
set.seed(n)
temp <- data_1 %>% select(-Subset) %>% slice_sample(prop = 1, replace = TRUE)
temp$Subset <- if_else(1:420<=nrow(m.m1_train) ,"Train", "Test")
# Set targets
Y_samp       <- temp$Y
Y_train_samp <- temp[temp$Subset == "Train" ,]$Y
Y_test_samp  <- temp[temp$Subset == "Test"  ,]$Y
# Cycle through k clusters to fit
for (k in 1:num_k) {
# Fit K-means routine
k.means <- kmeans(temp %>% select(X1,X2),
centers = k,
iter.max = 1000,
nstart = 50,
algorithm = "Lloyd",
trace = FALSE)
# eval(parse(text = paste0("temp$Clust_",k,"_ <- k.means$cluster %>% as.factor()")))
temp$Clust_ <- k.means$cluster %>% as.factor()
if (k == 1) {
temp$Clust_ <- NULL
}
# Set model matrix
m.m3       <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,
data = temp %>% select(-Y,-Subset))
head(m.m3, n = 5)
# Continue this logic to make train and test subsets
m.m3_train <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = temp %>%
filter(Subset == "Train") %>% select(-Y,-Subset))
m.m3_test  <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = temp %>%
filter(Subset == "Test") %>%  select(-Y,-Subset))
# Fit Cross-Validated Model
set.seed(seed_cv)
fit3_cv           <- cv.glmnet(m.m3_train, Y_train_samp, alpha=1, nfolds = 10)
fit3_parsimonious <- glmnet( m.m3_train,
Y_train_samp,
family = "gaussian",
weights = NULL,
offset = NULL,
alpha = 1,
lambda = fit3_cv$lambda.1se
)
# In-sample RMSE
pred.fit3_train <- predict(fit3_parsimonious, newx = m.m3_train)
rmse3_train = rmse_fun(Y_train_samp, pred.fit3_train)
# Out-of-sample RMSE
pred.fit3_test <- predict(fit3_parsimonious, newx = m.m3_test)
rmse3_test = rmse_fun(Y_test_samp, pred.fit3_test)
# Choose out-of-sample (test data) RMSE
rec_vec[n,k] <- min(rmse3_test)
}
}
set.seed(333)
num_n <-  10
num_k <-  10
rec_vec = matrix(nrow = num_n,
ncol = num_k)
# Cycle through re-samples n
for (n in 1:num_n) {
# Chose different seed for each random subset
set.seed(n)
temp <- data_1 %>% select(-Subset) %>% slice_sample(prop = 1, replace = TRUE)
temp$Subset <- if_else(1:420<=nrow(m.m1_train) ,"Train", "Test")
# Set targets
Y_samp       <- temp$Y
Y_train_samp <- temp[temp$Subset == "Train" ,]$Y
Y_test_samp  <- temp[temp$Subset == "Test"  ,]$Y
# Cycle through k clusters to fit
for (k in 1:num_k) {
# Fit K-means routine
k.means <- kmeans(temp %>% select(X1,X2),
centers = k,
iter.max = 1000,
nstart = 50,
algorithm = "Lloyd",
trace = FALSE)
# eval(parse(text = paste0("temp$Clust_",k,"_ <- k.means$cluster %>% as.factor()")))
temp$Clust_ <- k.means$cluster %>% as.factor()
if (k == 1) {
temp$Clust_ <- NULL
}
# Set model matrix
m.m3       <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1,
data = temp %>% select(-Y,-Subset))
head(m.m3, n = 5)
# Continue this logic to make train and test subsets
m.m3_train <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = temp %>%
filter(Subset == "Train") %>% select(-Y,-Subset))
m.m3_test  <- model.matrix(~ .*. + I(X1^2) + I(X2^2) -1, data = temp %>%
filter(Subset == "Test") %>%  select(-Y,-Subset))
# Fit Cross-Validated Model
set.seed(seed_cv)
fit3_cv           <- cv.glmnet(m.m3_train, Y_train_samp, alpha=1, nfolds = 10)
fit3_parsimonious <- glmnet( m.m3_train,
Y_train_samp,
family = "gaussian",
weights = NULL,
offset = NULL,
alpha = 1,
lambda = fit3_cv$lambda.1se
)
# In-sample RMSE
pred.fit3_train <- predict(fit3_parsimonious, newx = m.m3_train)
rmse3_train = rmse_fun(Y_train_samp, pred.fit3_train)
# Out-of-sample RMSE
pred.fit3_test <- predict(fit3_parsimonious, newx = m.m3_test)
rmse3_test = rmse_fun(Y_test_samp, pred.fit3_test)
# Choose out-of-sample (test data) RMSE
rec_vec[n,k] <- min(rmse3_test)
}
}
colVars
?colVars
# Capture the mean and standard deviation for each K
vec_mean <- colMeans(rec_vec)
vec_sd   <- colVars(rec_vec) %>% sqrt()
# Combine k, mean(RMSE) and sd(RMSE)
dat <- tibble("k" = 1:10,
"mean" = vec_mean,
"sd" = vec_sd,
)
